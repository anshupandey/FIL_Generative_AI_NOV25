{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9sVXN1ST4ZU"
      },
      "source": [
        "# LangGraph Exercise: Building Stateful LLM Applications with Azure OpenAI\n",
        "\n",
        "This exercise is designed for experienced technical professionals to gain hands-on experience with **LangGraph**, a library for building robust, stateful, and multi-actor applications with Large Language Models (LLMs). We will use **Azure OpenAI Service** as our LLM provider.\n",
        "\n",
        "## Goal\n",
        "Implement two core LangGraph patterns:\n",
        "1.  **Part A: Conditional Graph (Client Query Routing):** Create a dynamic workflow that routes a user query to one of three specialized agents based on the query's content.\n",
        "2.  **Part B: Static Graph (Compliance Report Generation):** Build a fixed, sequential workflow for automated document assembly.\n",
        "\n",
        "## Prerequisites\n",
        "1.  Python environment with the following packages installed: `langchain`, `langgraph`, `langchain-openai`, `pydantic`.\n",
        "2.  Access to an Azure OpenAI Service instance with a deployment named `gpt-4.1-mini`.\n",
        "3.  Your Azure OpenAI API Key and Endpoint."
      ],
      "id": "W9sVXN1ST4ZU"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai langgraph pydantic python-dotenv openai langsmith --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFXpvnTxVdxk",
        "outputId": "66dac9c1-2ed5-42bb-fe27-83ada8f91e18"
      },
      "id": "JFXpvnTxVdxk",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration and Imports\n",
        "import os\n",
        "from typing import Literal, TypedDict\n",
        "\n",
        "# Core LangChain/LangGraph imports\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taEoQVwGVl2N",
        "outputId": "7f254a6a-5f04-4c4b-9a15-920fcad56a2c"
      },
      "id": "taEoQVwGVl2N",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-pYWDXzT4ZW",
        "outputId": "c6048685-1a10-4114-ae76-7a419f269401"
      },
      "source": [
        "# --- Azure OpenAI Configuration ---\n",
        "# NOTE: Replace these placeholders with your actual Azure OpenAI credentials.\n",
        "# For a corporate environment, these would typically be loaded from a secure vault or environment variables.\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.environ.get(\"LANGSMITH_API_KEY\", \"xxxxxxxxxxxxxx\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://eastus.api.cognitive.microsoft.com/\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"xxxxxxxxxx\")\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ.get(\"OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "\n",
        "\n",
        "AZURE_OPENAI_DEPLOYMENT = \"gpt-4.1-mini\"\n",
        "\n",
        "\n",
        "print(\"Environment variables configured (using placeholders).\")\n",
        "print(f\"LLM Deployment: {AZURE_OPENAI_DEPLOYMENT}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables configured (using placeholders).\n",
            "LLM Deployment: gpt-4.1-mini\n"
          ]
        }
      ],
      "id": "z-pYWDXzT4ZW",
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSccdTVFT4ZW",
        "outputId": "e5612785-b687-451c-e3f3-0e301166dd2b"
      },
      "source": [
        "# Initialize AzureChatOpenAI LLM\n",
        "try:\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_deployment=AZURE_OPENAI_DEPLOYMENT,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(\"AzureChatOpenAI LLM initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not initialize AzureChatOpenAI. Please check your API key, endpoint, and deployment name: {e}\")\n",
        "    print(\"The notebook will continue, but LLM-dependent cells will use placeholders or default to a fallback route.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AzureChatOpenAI LLM initialized successfully.\n"
          ]
        }
      ],
      "id": "TSccdTVFT4ZW",
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSUXAy_ZT4ZW"
      },
      "source": [
        "## Part A: Client Query Routing Exercise\n",
        "\n",
        "In this exercise, we simulate a financial services firm where incoming client queries need to be routed to the correct specialist team: **General Inquiry**, **Policy Management**, or **Investment Strategy**. This is a classic use case for a **conditional graph** in LangGraph.\n",
        "\n",
        "The core component is a **Router Node** that uses the LLM's structured output capabilities to make a routing decision."
      ],
      "id": "wSUXAy_ZT4ZW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define State, Pydantic Schema for Routing, and the Router Node Function\n",
        "# 1. Define the State\n",
        "# TypedDict is used to define the state for the graph, avoiding full OOP classes.\n",
        "class RouterState(TypedDict):\n",
        "    query: str\n",
        "    route: Literal[\"general\", \"policy\", \"investment\"]\n",
        "    response: str\n",
        "\n",
        "# 2. Define the Router Schema (Pydantic)\n",
        "class RouteDecision(BaseModel):\n",
        "    \"\"\"The decision on which specialist workflow should handle the client query.\"\"\"\n",
        "    route: Literal[\"general\", \"policy\", \"investment\"] = Field(\n",
        "        ...,\n",
        "        description=\"The determined route for the query. Must be one of: general, policy, investment.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "bQT8_OLLWOpX"
      },
      "id": "bQT8_OLLWOpX",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define the Router Prompt and Chain\n",
        "router_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert financial query router. Analyze the user's query and determine the most appropriate specialist team. Your output MUST be a JSON object conforming to the provided schema.\"),\n",
        "    (\"human\", \"Client Query: {query}\")\n",
        "])"
      ],
      "metadata": {
        "id": "RbFyGIaiWRX2"
      },
      "id": "RbFyGIaiWRX2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW0nEVqQT4ZW"
      },
      "source": [
        "# 4. Define the Router Node Function\n",
        "def route_query(state: RouterState) -> RouterState:\n",
        "    \"\"\"Determines the next step based on the query content.\"\"\"\n",
        "    print(f\"--- Router Node: Analyzing Query: {state['query']} ---\")\n",
        "\n",
        "    try:\n",
        "        # Bind the Pydantic schema to the LLM for structured output\n",
        "        router_chain = router_prompt | llm.with_structured_output(RouteDecision)\n",
        "        decision = router_chain.invoke({\"query\": state[\"query\"]})\n",
        "        route = decision.route\n",
        "        print(f\"--- Router Decision: Route to '{route}' ---\")\n",
        "        return {\"route\": route}\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in router chain: {e}. Defaulting to 'general'.\")\n",
        "        # Fallback to general in case of error\n",
        "        return {\"route\": \"general\"}"
      ],
      "outputs": [],
      "id": "dW0nEVqQT4ZW",
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cO87o8_T4ZW"
      },
      "source": [
        "# Define the three Nodes and the Conditional Edge Function\n",
        "# Define the three specialist nodes.\n",
        "# For this exercise, they will simply return a simulated response.\n",
        "\n",
        "def general_node(state: RouterState) -> RouterState:\n",
        "    print(\"--- General Node: Processing Inquiry ---\")\n",
        "    response = f\"Response from General Inquiry: Thank you for your general question about our services. We are looking into your request: '{state['query']}'.\"\n",
        "    return {\"response\": response}\n",
        "\n",
        "def policy_node(state: RouterState) -> RouterState:\n",
        "    print(\"--- Policy Node: Processing Inquiry ---\")\n",
        "    response = f\"Response from Policy Management: Your query regarding your policy: '{state['query']}' has been forwarded to a Policy Specialist. Expect a detailed response within 24 hours.\"\n",
        "    return {\"response\": response}\n",
        "\n",
        "def investment_node(state: RouterState) -> RouterState:\n",
        "    print(\"--- Investment Node: Processing Inquiry ---\")\n",
        "    response = f\"Response from Investment Strategy: We are analyzing your investment-related query: '{state['query']}'. Please note that investment advice is subject to market conditions.\"\n",
        "    return {\"response\": response}\n",
        "\n",
        "# Define the conditional function for the router\n",
        "def get_next_step(state: RouterState) -> str:\n",
        "    \"\"\"The conditional function that maps the route to the next node name.\"\"\"\n",
        "    if state[\"route\"] == \"general\":\n",
        "        return \"general_node\"\n",
        "    elif state[\"route\"] == \"policy\":\n",
        "        return \"policy_node\"\n",
        "    elif state[\"route\"] == \"investment\":\n",
        "        return \"investment_node\"\n",
        "    else:\n",
        "        # Should not happen if Pydantic is enforced, but good for robustness\n",
        "        return \"general_node\""
      ],
      "outputs": [],
      "id": "1cO87o8_T4ZW",
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkdgKXMTT4ZX",
        "outputId": "f11c726d-eaf5-4169-e5b3-1044941e5066"
      },
      "source": [
        "# Build and Compile the LangGraph for Routing\n",
        "# 1. Initialize the StateGraph\n",
        "workflow = StateGraph(RouterState)\n",
        "\n",
        "# 2. Add the nodes\n",
        "workflow.add_node(\"router\", route_query)\n",
        "workflow.add_node(\"general_node\", general_node)\n",
        "workflow.add_node(\"policy_node\", policy_node)\n",
        "workflow.add_node(\"investment_node\", investment_node)\n",
        "\n",
        "# 3. Set the entry point\n",
        "workflow.set_entry_point(\"router\")\n",
        "\n",
        "# 4. Add conditional edges from the router\n",
        "# The conditional edge uses the get_next_step function to decide the next node\n",
        "workflow.add_conditional_edges(\n",
        "    \"router\",\n",
        "    get_next_step,\n",
        "    {\n",
        "        \"general_node\": \"general_node\",\n",
        "        \"policy_node\": \"policy_node\",\n",
        "        \"investment_node\": \"investment_node\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 5. Add finish edges from the agents\n",
        "workflow.add_edge(\"general_node\", END)\n",
        "workflow.add_edge(\"policy_node\", END)\n",
        "workflow.add_edge(\"investment_node\", END)\n",
        "\n",
        "# 6. Compile the graph\n",
        "app_router = workflow.compile()\n",
        "\n",
        "print(\"LangGraph for Client Query Routing compiled successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph for Client Query Routing compiled successfully.\n"
          ]
        }
      ],
      "id": "RkdgKXMTT4ZX",
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3CC_p00T4ZX",
        "outputId": "2714f7cc-bf9d-4b2e-9ad4-477f1fd547c2"
      },
      "source": [
        "# Test the Routing Graph with three distinct queries\n",
        "# Test Case 1: General Inquiry\n",
        "query_1 = \"What are your office hours next week?\"\n",
        "print(f\" --- Running Test Case 1: {query_1} ---\")\n",
        "result_1 = app_router.invoke({\"query\": query_1})\n",
        "print(f\"Final State Response: {result_1['response']}\")\n",
        "print(f\"Final State Route: {result_1['route']}\")\n",
        "\n",
        "# Test Case 2: Policy Management\n",
        "query_2 = \"I need to update the beneficiary on my term life insurance policy.\"\n",
        "print(f\" --- Running Test Case 2: {query_2} ---\")\n",
        "result_2 = app_router.invoke({\"query\": query_2})\n",
        "print(f\"Final State Response: {result_2['response']}\")\n",
        "print(f\"Final State Route: {result_2['route']}\")\n",
        "\n",
        "# Test Case 3: Investment Strategy\n",
        "query_3 = \"What is your current outlook on the S&P 500 for the next quarter?\"\n",
        "print(f\" --- Running Test Case 3: {query_3} ---\")\n",
        "result_3 = app_router.invoke({\"query\": query_3})\n",
        "print(f\"Final State Response: {result_3['response']}\")\n",
        "print(f\"Final State Route: {result_3['route']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --- Running Test Case 1: What are your office hours next week? ---\n",
            "--- Router Node: Analyzing Query: What are your office hours next week? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py:1936: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Router Decision: Route to 'general' ---\n",
            "--- General Node: Processing Inquiry ---\n",
            "Final State Response: Response from General Inquiry: Thank you for your general question about our services. We are looking into your request: 'What are your office hours next week?'.\n",
            "Final State Route: general\n",
            " --- Running Test Case 2: I need to update the beneficiary on my term life insurance policy. ---\n",
            "--- Router Node: Analyzing Query: I need to update the beneficiary on my term life insurance policy. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py:1936: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Router Decision: Route to 'policy' ---\n",
            "--- Policy Node: Processing Inquiry ---\n",
            "Final State Response: Response from Policy Management: Your query regarding your policy: 'I need to update the beneficiary on my term life insurance policy.' has been forwarded to a Policy Specialist. Expect a detailed response within 24 hours.\n",
            "Final State Route: policy\n",
            " --- Running Test Case 3: What is your current outlook on the S&P 500 for the next quarter? ---\n",
            "--- Router Node: Analyzing Query: What is your current outlook on the S&P 500 for the next quarter? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py:1936: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Router Decision: Route to 'investment' ---\n",
            "--- Investment Node: Processing Inquiry ---\n",
            "Final State Response: Response from Investment Strategy: We are analyzing your investment-related query: 'What is your current outlook on the S&P 500 for the next quarter?'. Please note that investment advice is subject to market conditions.\n",
            "Final State Route: investment\n"
          ]
        }
      ],
      "id": "c3CC_p00T4ZX",
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYceZT5gT4ZX"
      },
      "source": [
        "### Result Interpretation for Part A\n",
        "\n",
        "The execution trace clearly demonstrates the power of **conditional routing** in LangGraph.\n",
        "\n",
        "1.  The graph starts at the `router` node.\n",
        "2.  The `router` node uses the LLM with a Pydantic schema to classify the query and determine the `route` (e.g., 'investment').\n",
        "3.  The `add_conditional_edges` function then uses the `get_next_step` function to read the `route` from the state and dynamically transition to the correct agent node (`investment_agent`).\n",
        "4.  The selected agent node executes and the graph terminates (`END`).\n",
        "\n",
        "This pattern is crucial for building complex, multi-agent systems where different tasks require specialized logic or tools."
      ],
      "id": "oYceZT5gT4ZX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKJOEo4T4ZX"
      },
      "source": [
        "## Part B: Automate Compliance Report Generation Exercise\n",
        "\n",
        "This exercise demonstrates a **static, sequential graph** used for a fixed workflow, such as automated document assembly. We will create a chain to generate a compliance report in three fixed steps: Data Collection, Risk Analysis, and Report Assembly."
      ],
      "id": "zXKJOEo4T4ZX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define State and the three Sequential Node Functions\n",
        "# 1. Define the State for the Compliance Report\n",
        "class ReportState(TypedDict):\n",
        "    report_title: str\n",
        "    data_summary: str\n",
        "    risk_analysis: str\n",
        "    final_report: str"
      ],
      "metadata": {
        "id": "CLvwRNMMXwSy"
      },
      "id": "CLvwRNMMXwSy",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYXz1UkJT4ZX"
      },
      "source": [
        "\n",
        "# 2. Define the Sequential Node Functions\n",
        "\n",
        "def data_collection_node(state: ReportState) -> ReportState:\n",
        "    \"\"\"Simulates the first step: collecting and summarizing compliance data.\"\"\"\n",
        "    print(\"--- Step 1: Data Collection Node Executed ---\")\n",
        "    title = state[\"report_title\"]\n",
        "\n",
        "    # Simulated data collection result\n",
        "    summary = f\"Summary for {title}: The Q3 2025 transaction log shows 1,245,890 trades. 99.8% were executed within the 100ms latency target. Two minor breaches of the insider trading policy were flagged and resolved internally.\"\n",
        "\n",
        "    return {\"data_summary\": summary}\n",
        "\n",
        "def risk_analysis_node(state: ReportState) -> ReportState:\n",
        "    \"\"\"Uses the data summary to perform a risk analysis (LLM-powered).\"\"\"\n",
        "    print(\"--- Step 2: Risk Analysis Node Executed ---\")\n",
        "\n",
        "    # Check if LLM is initialized before calling\n",
        "    if llm is None:\n",
        "        print(\"LLM not initialized. Skipping LLM call and using placeholder analysis.\")\n",
        "        analysis_result = \"Placeholder Risk Analysis: Due to uninitialized LLM, a full analysis could not be performed. The system flagged two minor policy breaches which require follow-up.\"\n",
        "        return {\"risk_analysis\": analysis_result}\n",
        "\n",
        "    # LLM Prompt for Risk Analysis\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a senior compliance officer. Based on the provided data summary, write a concise, professional risk analysis section for a compliance report. Focus on key risks and mitigation.\"),\n",
        "        (\"human\", \"Data Summary: {data_summary}\")\n",
        "    ])\n",
        "\n",
        "    analysis_chain = analysis_prompt | llm\n",
        "\n",
        "    # Invoke the LLM\n",
        "    analysis_result = analysis_chain.invoke({\"data_summary\": state[\"data_summary\"]}).content\n",
        "\n",
        "    return {\"risk_analysis\": analysis_result}\n",
        "\n",
        "def report_assembly_node(state: ReportState) -> ReportState:\n",
        "    \"\"\"Combines all sections into the final report document.\"\"\"\n",
        "    print(\"--- Step 3: Report Assembly Node Executed ---\")\n",
        "\n",
        "    final_report = f\"\"\"\n",
        "# {state[\"report_title\"]}\n",
        "\n",
        "## 1. Executive Data Summary\n",
        "{state[\"data_summary\"]}\n",
        "\n",
        "## 2. Risk Analysis\n",
        "{state[\"risk_analysis\"]}\n",
        "\n",
        "---\n",
        "*Report Generated by Automated Compliance Chain.*\"\"\"\n",
        "    return {\"final_report\": final_report}"
      ],
      "outputs": [],
      "id": "QYXz1UkJT4ZX",
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMunl2a7T4ZX",
        "outputId": "7535c1a1-5737-4a00-9551-a1c26dd9897b"
      },
      "source": [
        "# Build and Compile the Static LangGraph Chain\n",
        "# 1. Initialize the StateGraph\n",
        "workflow_report = StateGraph(ReportState)\n",
        "\n",
        "# 2. Add the nodes\n",
        "workflow_report.add_node(\"data_collection\", data_collection_node)\n",
        "workflow_report.add_node(\"risk_analysis\", risk_analysis_node)\n",
        "workflow_report.add_node(\"report_assembly\", report_assembly_node)\n",
        "\n",
        "# 3. Set the entry point\n",
        "workflow_report.set_entry_point(\"data_collection\")\n",
        "\n",
        "# 4. Add sequential edges (static chain)\n",
        "workflow_report.add_edge(\"data_collection\", \"risk_analysis\")\n",
        "workflow_report.add_edge(\"risk_analysis\", \"report_assembly\")\n",
        "\n",
        "# 5. Add finish edge\n",
        "workflow_report.add_edge(\"report_assembly\", END)\n",
        "\n",
        "# 6. Compile the graph\n",
        "app_report = workflow_report.compile()\n",
        "\n",
        "print(\"LangGraph for Compliance Report Generation compiled successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph for Compliance Report Generation compiled successfully.\n"
          ]
        }
      ],
      "id": "NMunl2a7T4ZX",
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3qddCoXT4ZX",
        "outputId": "6e3fbb98-0c33-4194-ad68-46831c73603d"
      },
      "source": [
        "# Run the Static Graph and print the final report\n",
        "# Initial state with the report title\n",
        "initial_state = {\n",
        "    \"report_title\": \"Q3 2025 Automated Compliance Report\",\n",
        "    \"data_summary\": \"\",\n",
        "    \"risk_analysis\": \"\",\n",
        "    \"final_report\": \"\"\n",
        "}\n",
        "\n",
        "print(f\" --- Running Compliance Report Generation for: {initial_state['report_title']} ---\")\n",
        "final_result = app_report.invoke(initial_state)\n",
        "\n",
        "print(\" --- Generated Final Report ---\")\n",
        "print(final_result[\"final_report\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --- Running Compliance Report Generation for: Q3 2025 Automated Compliance Report ---\n",
            "--- Step 1: Data Collection Node Executed ---\n",
            "--- Step 2: Risk Analysis Node Executed ---\n",
            "--- Step 3: Report Assembly Node Executed ---\n",
            " --- Generated Final Report ---\n",
            "\n",
            "# Q3 2025 Automated Compliance Report\n",
            "\n",
            "## 1. Executive Data Summary\n",
            "Summary for Q3 2025 Automated Compliance Report: The Q3 2025 transaction log shows 1,245,890 trades. 99.8% were executed within the 100ms latency target. Two minor breaches of the insider trading policy were flagged and resolved internally.\n",
            "\n",
            "## 2. Risk Analysis\n",
            "Risk Analysis:\n",
            "\n",
            "The Q3 2025 transaction data indicates a high level of operational efficiency, with 99.8% of trades executed within the established 100ms latency target, minimizing market risk related to execution delays. However, the identification of two minor insider trading policy breaches, although promptly resolved internally, highlights a residual compliance risk in information handling and employee conduct. To mitigate this, enhanced monitoring and targeted training on insider trading policies are recommended to prevent recurrence and strengthen the firm’s overall compliance posture.\n",
            "\n",
            "---\n",
            "*Report Generated by Automated Compliance Chain.*\n"
          ]
        }
      ],
      "id": "v3qddCoXT4ZX",
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9olDfi0T4ZX"
      },
      "source": [
        "### Result Interpretation for Part B\n",
        "\n",
        "This exercise demonstrates a **static, sequential workflow** where the execution path is fixed and predictable.\n",
        "\n",
        "1.  The graph starts at `data_collection`.\n",
        "2.  It proceeds directly to `risk_analysis` via a fixed edge.\n",
        "3.  It then proceeds to `report_assembly` via another fixed edge.\n",
        "4.  Each node updates the shared `ReportState` with its output, ensuring that the next node has the necessary context (e.g., `risk_analysis` uses `data_summary`).\n",
        "\n",
        "This pattern is ideal for reliable, multi-step processes like data pipelines, document generation, or fixed business logic where the flow does not depend on an LLM decision."
      ],
      "id": "g9olDfi0T4ZX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqEqBBlST4ZX"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This exercise successfully demonstrated the two fundamental patterns of LangGraph:\n",
        "\n",
        "| Pattern | LangGraph Feature | Use Case | Key Takeaway |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Part A: Client Query Routing** | Conditional Edges (`add_conditional_edges`) | Dynamic routing, multi-agent systems, tool selection. | LLMs can be used as intelligent routers to dynamically change the workflow path. |\n",
        "| **Part B: Compliance Report** | Static Edges (`add_edge`) | Fixed, sequential processes, document assembly, data pipelines. | LangGraph provides a robust framework for managing state and flow in predictable, multi-step tasks. |\n",
        "\n",
        "By mastering these two patterns, you are equipped to build sophisticated, production-ready LLM applications."
      ],
      "id": "lqEqBBlST4ZX"
    }
  ]
}