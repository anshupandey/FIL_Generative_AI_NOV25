{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "QOQ7oa5QW5QQ",
      "metadata": {
        "id": "QOQ7oa5QW5QQ"
      },
      "source": [
        "# ðŸš€ PromptOps with LangSmith and Azure OpenAI\n",
        "\n",
        "This Jupyter Notebook provides a step-by-step guide on implementing **Prompt Operations (PromptOps)** using **LangSmith** for prompt management, **LangChain** for orchestration, and **Azure OpenAI** for content generation. We will cover the entire lifecycle: prompt registration, modification, loading, generation and deletion.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running the code, ensure you have the following environment variables set up. These are crucial for connecting to LangSmith and Azure OpenAI.\n",
        "\n",
        "| Variable | Purpose | Example Value |\n",
        "| :--- | :--- | :--- |\n",
        "| `LANGSMITH_API_KEY` | Authentication for LangSmith | `lsv2_sk_...` |\n",
        "| `LANGSMITH_TRACING` | Name of the LangSmith project for tracing | `true` |\n",
        "| `AZURE_OPENAI_ENDPOINT` | Your Azure OpenAI Service endpoint | `https://your-resource.openai.azure.com/` |\n",
        "| `AZURE_OPENAI_API_KEY` | Your Azure OpenAI API key | `abcdef123456...` |\n",
        "| `OPENAI_API_VERSION` | Azure OpenAI API version | `2024-02-01` |\n",
        "\n",
        "The Azure OpenAI deployment name used in this tutorial is **`gpt-4.1-mini`**, as specified in the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nbYpQjkYeX6w",
      "metadata": {
        "id": "nbYpQjkYeX6w"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai langchain-core langsmith evaluate openai python-dotenv rouge_score --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YEhUR-pjnqMd",
      "metadata": {
        "id": "YEhUR-pjnqMd"
      },
      "outputs": [],
      "source": [
        "# 1. Setup and Imports\n",
        "import os\n",
        "from langsmith import Client\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from evaluate import load\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OozwaPZVnr1a",
      "metadata": {
        "id": "OozwaPZVnr1a"
      },
      "outputs": [],
      "source": [
        "# --- Environment Variable Setup (Replace with your actual values or set them in your environment) ---\n",
        "# NOTE: In a real environment, these should be set externally for security.\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.environ.get(\"LANGSMITH_API_KEY\", \"xxxxxxxxxxxxxxxxx\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://eastus.api.cognitive.microsoft.com/\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ.get(\"AZURE_OPENAI_API_KEY\", \"xxxxxxxxxxxxx\")\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ.get(\"OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJZer7DfoDcy",
      "metadata": {
        "id": "TJZer7DfoDcy"
      },
      "outputs": [],
      "source": [
        "# Initialize LangSmith Client\n",
        "client = Client()\n",
        "print(\"LangSmith Client Initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "vo2EgAHZW5QR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo2EgAHZW5QR",
        "outputId": "61aa6724-0cb2-44e0-9ad7-e84dd397983d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith Client Initialized.\n",
            "AzureChatOpenAI Model Initialized with deployment: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# Initialize Azure OpenAI Model\n",
        "try:\n",
        "    llm = AzureChatOpenAI(\n",
        "        openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
        "        azure_deployment=\"gpt-4.1-mini\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(\"AzureChatOpenAI Model Initialized with deployment: gpt-4.1-mini\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not initialize AzureChatOpenAI. Please ensure all environment variables are set correctly. Error: {e}\")\n",
        "    llm = None # Set to None if initialization fails\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7Nv6W91W5QS",
      "metadata": {
        "id": "c7Nv6W91W5QS"
      },
      "source": [
        "## 1. Prompt Registration (Pushing a Prompt)\n",
        "\n",
        "LangSmith allows you to register and version your prompts programmatically using the `client.push_prompt()` method. We will create a simple prompt for generating short, professional summaries of technical concepts.\n",
        "\n",
        "The prompt name we will use is **`technical-summary-generator`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "NaKrmmsaW5QS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaKrmmsaW5QS",
        "outputId": "9bd16ed8-cdc8-4aea-9b36-1d3a6451e76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully registered prompt 'technical-summary-generator' (Version 1).\n",
            "View in LangSmith: https://smith.langchain.com/prompts/technical-summary-generator/3528168c?organizationId=d902fd7a-b325-505a-adcb-ea98ad22246a\n"
          ]
        }
      ],
      "source": [
        "# Define the initial prompt (Version 1)\n",
        "prompt_name = \"technical-summary-generator\"\n",
        "initial_template = (\n",
        "    \"You are a professional technical writer. Your task is to write a concise, one-paragraph summary of the following technical concept: {concept}. \"\n",
        "    \"The summary must be easy to understand for a non-technical audience.\"\n",
        ")\n",
        "\n",
        "initial_prompt = ChatPromptTemplate.from_template(initial_template)\n",
        "\n",
        "# Push the initial prompt to LangSmith\n",
        "try:\n",
        "    url = client.push_prompt(prompt_name, object=initial_prompt)\n",
        "    print(f\"Successfully registered prompt '{prompt_name}' (Version 1).\")\n",
        "    print(f\"View in LangSmith: {url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing prompt: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hpXPEfSYW5QS",
      "metadata": {
        "id": "hpXPEfSYW5QS"
      },
      "source": [
        "## 2. Prompt Modification (Pushing a New Version)\n",
        "\n",
        "To modify the prompt, we simply define a new version and use `client.push_prompt()` again with the same name. LangSmith automatically handles the versioning, creating a new commit for the change.\n",
        "\n",
        "**Modification**: We will add a constraint to include a real-world example in the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "nac71c96W5QT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nac71c96W5QT",
        "outputId": "086be434-fa08-429e-deb2-f2e5898bc97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully updated prompt 'technical-summary-generator' (Version 2).\n",
            "View in LangSmith: https://smith.langchain.com/prompts/technical-summary-generator/0438a4a7?organizationId=d902fd7a-b325-505a-adcb-ea98ad22246a\n"
          ]
        }
      ],
      "source": [
        "# Define the modified prompt (Version 2)\n",
        "modified_template = (\n",
        "    \"You are a professional technical writer. Your task is to write a concise, one-paragraph summary of the following technical concept: {concept}. \"\n",
        "    \"The summary must be easy to understand for a non-technical audience and **must include a simple, real-world example**.\"\n",
        ")\n",
        "\n",
        "modified_prompt = ChatPromptTemplate.from_template(modified_template)\n",
        "\n",
        "# Push the modified prompt to LangSmith\n",
        "try:\n",
        "    url = client.push_prompt(prompt_name, object=modified_prompt)\n",
        "    print(f\"Successfully updated prompt '{prompt_name}' (Version 2).\")\n",
        "    print(f\"View in LangSmith: {url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing modified prompt: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yfnjjv3rW5QT",
      "metadata": {
        "id": "Yfnjjv3rW5QT"
      },
      "source": [
        "## 3. Loading the Prompt (Pulling a Prompt)\n",
        "\n",
        "We can load the latest version of the prompt using `client.pull_prompt()`. This ensures our application always uses the most up-to-date, tested prompt from the hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9CWRAi9cW5QT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CWRAi9cW5QT",
        "outputId": "8939d8bf-f0fe-44f1-bdc8-22d5d2bf4fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully pulled latest prompt: 'technical-summary-generator'\n",
            "\n",
            "--- Pulled Prompt Template ---\n",
            "You are a professional technical writer. Your task is to write a concise, one-paragraph summary of the following technical concept: {concept}. The summary must be easy to understand for a non-technical audience and **must include a simple, real-world example**.\n"
          ]
        }
      ],
      "source": [
        "# Pull the latest prompt from LangSmith\n",
        "try:\n",
        "    latest_prompt = client.pull_prompt(prompt_name)\n",
        "    print(f\"Successfully pulled latest prompt: '{prompt_name}'\")\n",
        "    print(\"\\n--- Pulled Prompt Template ---\")\n",
        "    print(latest_prompt.messages[0].prompt.template)\n",
        "except Exception as e:\n",
        "    print(f\"Error pulling prompt: {e}\")\n",
        "    latest_prompt = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jLtjt9kRW5QT",
      "metadata": {
        "id": "jLtjt9kRW5QT"
      },
      "source": [
        "## 4. Content Generation with Azure OpenAI\n",
        "\n",
        "Now we combine the loaded prompt with our `AzureChatOpenAI` model to create a simple LangChain Expression Language (LCEL) chain for content generation. The execution of this chain will automatically be traced and logged in LangSmith, linking the generated output back to the specific prompt version used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "sxYxoM84W5QT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxYxoM84W5QT",
        "outputId": "64b920c7-193d-4d25-91bd-87beae8a694c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating summary for: Quantum Computing...\n",
            "\n",
            "--- Generated Summary ---\n",
            "Quantum computing is a new type of computing that uses the strange behavior of tiny particles, called quantum bits or qubits, to solve problems much faster than regular computers. Unlike traditional bits that are either 0 or 1, qubits can be both at the same time, allowing quantum computers to explore many possibilities simultaneously. Imagine trying to find the fastest route through a city with many roads: a regular computer checks each path one by one, but a quantum computer can consider all routes at once, quickly finding the best option. This technology has the potential to revolutionize fields like medicine, finance, and cryptography by solving complex problems that are currently impossible for ordinary computers.\n",
            "\n",
            "Check LangSmith for the trace of this run!\n"
          ]
        }
      ],
      "source": [
        "if llm and latest_prompt:\n",
        "    # Create the LCEL chain: Prompt -> LLM -> Output Parser\n",
        "    chain = latest_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Define the input concept\n",
        "    input_concept = \"Quantum Computing\"\n",
        "\n",
        "    print(f\"Generating summary for: {input_concept}...\")\n",
        "\n",
        "    # Invoke the chain\n",
        "    try:\n",
        "        response = chain.invoke({\"concept\": input_concept})\n",
        "        print(\"\\n--- Generated Summary ---\")\n",
        "        print(response)\n",
        "        print(\"\\nCheck LangSmith for the trace of this run!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during content generation: {e}\")\n",
        "        response = \"Generation Failed\"\n",
        "else:\n",
        "    response = \"Generation Skipped due to setup errors.\"\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3BKTn9_Kmn7Q",
      "metadata": {
        "id": "3BKTn9_Kmn7Q"
      },
      "source": [
        "## 5. List, delete, and like prompts\n",
        "\n",
        "You can also list, delete, and like/unlike prompts using the list prompts, delete prompt, like prompt and unlike prompt methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o2APmHmfmx0M",
      "metadata": {
        "id": "o2APmHmfmx0M"
      },
      "outputs": [],
      "source": [
        "# List all prompts in my workspace\n",
        "prompts = client.list_prompts()\n",
        "\n",
        "# List my private prompts that include \"joke\"\n",
        "prompts = client.list_prompts(query=\"summarize\", is_public=False)\n",
        "\n",
        "# Delete a prompt\n",
        "client.delete_prompt(prompt_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "oLx-PcIunGWI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLx-PcIunGWI",
        "outputId": "aa484524-00b1-45e5-b801-c66824b621b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'likes': 2}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Like a prompt\n",
        "client.like_prompt(\"efriis/my-first-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "LWHpK0mKnIy2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWHpK0mKnIy2",
        "outputId": "95579bcb-d39f-48ed-9419-a08430fe4243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'likes': 1}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unlike a prompt\n",
        "client.unlike_prompt(\"efriis/my-first-prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-8yDJbG_W5QT",
      "metadata": {
        "id": "-8yDJbG_W5QT"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial demonstrated a complete PromptOps workflow:\n",
        "\n",
        "1.  **Registration & Modification**: Using `client.push_prompt()` to version control prompts in LangSmith.\n",
        "2.  **Loading**: Using `client.pull_prompt()` to ensure the application uses the latest, approved version.\n",
        "3.  **Generation**: Integrating the prompt with `AzureChatOpenAI` via a LangChain LCEL chain, with automatic tracing to LangSmith.\n",
        "\n",
        "This workflow is the foundation for robust, and iterative development of LLM applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "KgLI7EpRgf-U",
      "metadata": {
        "id": "KgLI7EpRgf-U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
